---
title: Human Google Answer
description: "A brief hypothetical thought experiment on the value of our own thinking."
slug: human-google-answer
date: 2023-08-18 00:00:00+0000
image: girraween_18_03_2023.jpeg
layout: "simple"
categories:
    - Short Thoughts
tags:
    - Short
    - Essay
draft: false
---

A man has device that grants him correct answers to all questions or a man has to work out the answers to his questions himself? What would you like more: google maps to tell you how to get to your destination or a refedex and your own solution? Are we trading cognitive exercise for convenience or simply reallocating brain power to something else? Taking this to the limit, do we find ourselves in a situation in which our own problem solving is no longer necessary at all, nullifying the need to get answers to our questions in the first place?

A man has an app, like Notion, that will organise his weekly plan in an optimal way to achieve his work, exercise, relationship etc needs. Would you use it to save time that would be spent otherwise planning your week? Taken to the extreme, imagine having an app or system that would plan your entire life, no input from you required at all. The application would optimise for longevity, life satisfaction, harmony with others and the environment. Sounds good right? But at what stage would you want some control or input into how this app makes choices for you? If you have spent your whole life having your schedule and choices being made by an application, would you actually be in a good position to make any decision? Automony over daily life might be prequisite for happiness in itself.

I spoke with my wife about this issue. By way of example, I explained to her that she had to choose between having a machine that could make any clothes she desired for her, ball gowns, exercise clothes etc or being able to have the skill to make her own clothes. It was the machine that could make everything that was her pick as it was ultimately the clothes that she wanted not the underlying skill that made them. Next, she brought up her own example of having AI, like Chat GPT, write a scientific journal article for you. This she thought was absurd as you would need to be in a position to understand the how the content was dervived in the article in order to defend it and use it to further your own research. I thought this was an illuminating point. The output of machines that find answers to our problems are acceptable in so far as we understand the output ourselves.

Fuel, the smelly liquid that comes out of a machine at the petrol station. Do I understand how I get this magical stuff that makes my car go to anywhere I road can take me? Clearly, the processes that get oil out of the ground and into my car are hugely complicated. Here, I think the distinction lies that fuel coming out of a spout doesn't make any cognitive decisions for me. It is a product that I use to power my journeys, not a delegated product of my arthimetic. After all this am I in a position to answer the hypothetical choice between using thinking machines or reasoning about things myself?

As of right now I think I value my reasoning more; exercising cognitive muscles to delay the decline in my brain's ability, than "optimal" results chucked out from an web application. This means I am okay with being wrong, having in complete or suboptimal results as a product of my own thinking and being okay with not having all the answers. Chat GPT could have wrote this, but that wasn't the point. 


